{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "closing-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#testing python\n",
    "\n",
    "\n",
    "an_int = 1 + 2\n",
    "print(an_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "int / int returns real number (float)\n",
    "\t  int // int returns int\n",
    "\t** = ^  \t4**2 = 4^2  \t5**8 = 5^8\n",
    "\t**(1/2) = sqrt\n",
    "\t+, -, %, * are all standard\n",
    "\tStr(234) makes 234 a string.\n",
    "\tConcatenation only works if both things are string.\n",
    "\t\t“A number is” + 4 doesn’t work. “A number is”, str(4) works\n",
    "\t\t“A number is”, “4” works.\n",
    "\tString.split() splits on whatever is in the parentheses, puts it in a list\n",
    "\tString.find(“an”) finds if something is inside that string. If returns -1, it isn’t there.\n",
    "\"\"\"\n",
    "\n",
    "another_int = 4**2\n",
    "print(another_int, \"should be 16\",end=\"\")\n",
    "print(\" That should be 16, and end=\\\"\\\" should bypass the default \\n in python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"this is a sentence we can work with, python is kool\"\n",
    "myList = sentence.split(\" \")\n",
    "print(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashmap(myHash) = sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "\n",
    "#put comments above stuff\n",
    "boo = True\n",
    "i = 0\n",
    "while boo == True:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    if i == 5:\n",
    "        print(\"yeah this works dawg\")\n",
    "        boo = False\n",
    "print(\"done\")\n",
    "for i in range(1, 11, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100, 69, -2):\n",
    "    print(i)\n",
    "print(range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data types\n",
    "types_of_data = \"int,float,string, long\"\n",
    "print(types_of_data.split)\n",
    "\n",
    "print(\"Enter a number\")\n",
    "line = input(\"\")\n",
    "line = int(line) + 2\n",
    "print(line,end=\"\")\n",
    "print(\" is your number plus 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a basic function\n",
    "\n",
    "x = int(input(\"Enter a number\"))\n",
    "y = int(input(\"Enter another number\"))\n",
    "def add(x, y):\n",
    "    answer = x + y\n",
    "    return \"Those numbers added are\", answer\n",
    "\n",
    "print(add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    if n==0: return 1\n",
    "    else: return factorial(n-1) * n\n",
    "    \n",
    "#Doesn't work because factorial makes phat numbers\n",
    "#print(\"{:e}\".format(factorial(200)))\n",
    "#If you get qwargs error it means \"key work arguments\"\n",
    "#key word arguments are start end and skip\n",
    "\n",
    "print(factorial(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(start = 1, end=10, skip = 1):\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 'egg', 45, 3.2, 'lmao', 6, 7, 4.5, 7, 8, 'lol']\n",
      "lol\n",
      "egg\n",
      "1\n",
      "[2, 'egg', 45]\n",
      "['egg', 45, 3.2]\n",
      "['egg', 3.2, 6]\n",
      "[2, 'egg', 45, 3.2, 'lmao', 6, 7, 4.5, 7, 8, 'lol']\n",
      "[1, 2, '32', 45, 3.2, 'lmao', 6, 7, 4.5, 7, 8, 'lol']\n"
     ]
    }
   ],
   "source": [
    "#data structures in python\n",
    "\"\"\"\n",
    "Lists: get/set,search,append/remove end, add/remove\n",
    "\"\"\"\n",
    "\n",
    "a_lst = [1, 2, \"egg\", 45, 3.2, \"lmao\", 6, 7, 4.5, 7, 8, \"lol\"]\n",
    "print(a_lst)\n",
    "print(a_lst[-1])\n",
    "print(a_lst[2])\n",
    "print(a_lst[0])\n",
    "print(a_lst[1:4])\n",
    "print(a_lst[2:5])\n",
    "print(a_lst[2:8:2]) #skip values\n",
    "print(a_lst[1:12])\n",
    "a_lst[2] = \"32\"\n",
    "print(a_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "methods/functions\n",
    "len(a_lst) gets you length\n",
    ".append adds onto end\n",
    ".extend(another_list) tacks on list to end of other list\n",
    ".remove removes first value\n",
    ".pop([1]) removes item from index 1\n",
    ".reverse self explanatory\n",
    "\"\"\"\n",
    "\n",
    "print(a_lst)\n",
    "a_lst.reverse()\n",
    "print(a_lst)\n",
    "print(len(a_lst))\n",
    "# doesn't work: print(a_lst.pop([2]))\n",
    "print(a_lst.remove(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4]\n",
    "print(list1)\n",
    "for v in list1:\n",
    "    v*=2\n",
    "print(list1)\n",
    "#Doesn't do anything to list, a local change\n",
    "while len(list1)<=28:\n",
    "    list1.extend(list1)\n",
    "    print(list1)\n",
    "    \n",
    "print(3 in list1)\n",
    "\n",
    "\n",
    "#sort\n",
    "list1.sort()\n",
    "print(list1)\n",
    "\n",
    "sentence = \"This is a sentence for us TO mEss WiTh. A A a a A a A\"\n",
    "print(list2)\n",
    "sentence = sentence.upper()\n",
    "list2 = sentence.split(\" \")\n",
    "print(list2)\n",
    "\n",
    "list2.sort()\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuples, kinda like lists\n",
    "#They are immutable, meaning they are hashable\n",
    "#created like lists but with () not []\n",
    "#Used for mapping\n",
    "\n",
    "my_tuple = (1, 2, 3)\n",
    "print(my_tuple)\n",
    "print(my_tuple[2])\n",
    "\n",
    "\n",
    "#can return multiple values\n",
    "def foo():\n",
    "    return 1, 2, 3\n",
    "\n",
    "print(foo())\n",
    "\n",
    "a, b, c = foo()\n",
    "\n",
    "print(a, \" \", b, \" \", c, \" \")\n",
    "print(\"let's reverse a and b\")\n",
    "#Can perform a swap without an intermediate value, pretty cool\n",
    "a,b = b,a\n",
    "print(a,b)\n",
    "\n",
    "\n",
    "#can do the same thing with a list\n",
    "\n",
    "my_dict = dict()\n",
    "my_tuple = (1, 2, 3, 4, 5)\n",
    "#my_dict[my_tuple] #not working\n",
    "#print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "list2 = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "list3 = [0, 1, 1, 2, 0, 4, 5, 4, 4, 1, 0]\n",
    "\n",
    "#this is clunky\n",
    "for i in range(0, len(list1)):\n",
    "    print(i)\n",
    "    sum += list1[i] * list2[i]\n",
    "print(sum)\n",
    "\n",
    "\n",
    "#must be the same length\n",
    "sum=0\n",
    "for(p) in zip(list1, list2):\n",
    "    print(p)\n",
    "\n",
    "for(v1, v2) in zip(list2, list1):\n",
    "    sum += v1*v2\n",
    "print(sum)\n",
    "\n",
    "#testing if github recognizes this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries\n",
    "myDict = {\"name\" : \"bob\", \"age\" : 33, 1: \"asdf\"}\n",
    "\n",
    "\n",
    "for d in myDict:\n",
    "    print(d)\n",
    "    \n",
    "print(myDict[\"name\"])\n",
    "print(myDict[1])\n",
    "\n",
    "\n",
    "age = str(myDict[\"age\"])\n",
    "print((\"Age = \" + age))\n",
    "\n",
    "\n",
    "for p in zip(range(0,10), \"my name\"):\n",
    "    print(p)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #Lists are not hashable, cannot be key\n",
    "print(myDict)\n",
    "#myDict[\"name\":str(\"Fred Dawg\")] should work but doesn't\n",
    "print(myDict)\n",
    "\n",
    "length = len(myDict)\n",
    "keys = myDict.keys()\n",
    "print(length)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map, range, list are things\n",
    "    \n",
    "myDict[\"NaMe\"] = \"Big Dawg Fred\"\n",
    "\n",
    "for mapping in myDict:\n",
    "    print(mapping)\n",
    "\n",
    "    #sets exist\n",
    "#based on hashing, immutable, fast\n",
    "\n",
    "mySet = set([1, 2, 3, 4, 5])\n",
    "anotherSet = {4, 3, 4, 5, 6, 1, 2, 3, 9, 68}\n",
    "print(anotherSet)\n",
    "print(mySet)\n",
    "\n",
    "print(\"Sets can't have duplicates \\n \\tThat's pretty cool\")\n",
    "\n",
    "print(3 in mySet)\n",
    "print(59 in mySet)\n",
    "\n",
    "#thirdSet = mySet + anotherSet Doesn't work\n",
    "thirdSet = mySet.union(anotherSet)\n",
    "print(thirdSet)\n",
    "\n",
    "#set operations:\n",
    "\"\"\"\n",
    "a<= proper subset, a>= superset, a|b = union, a-b difference,\n",
    "a^b=symmetric difference, s&b = intersection\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def cube(x):\n",
    "    return x**3\n",
    "\n",
    "myList = [1, 2, 3, 4, 5]\n",
    "for i in range(len(myList)):\n",
    "    print(myList[i], end=\" \")\n",
    "    myList[i] = cube(myList[i])\n",
    "    print(myList[i])\n",
    "    \n",
    "    \n",
    "#can also do:\n",
    "def do_a_thing(fn, the_list):\n",
    "    result = []\n",
    "    for value in the_list:\n",
    "        result.append(fn(value))\n",
    "    return result\n",
    "\n",
    "\n",
    "print(do_a_thing(square, myList))\n",
    "\n",
    "\n",
    "addOne = (lambda x : x-2)\n",
    "print(do_a_thing(addOne, myList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [2, 4, 6, 8, 10, 2, 4, 1, 3, 7, 34, 7, 425, 43, 2, 46]\n",
    "\n",
    "#print(list(map\n",
    "\n",
    "#filter\n",
    "#not working but should\n",
    "#print(list(filter(Lambda x: x%2 == 0, myList)))\n",
    "\n",
    "\n",
    "squares = [x**2 for x in myList]\n",
    "print(squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-shame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with two parameters\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel?\n",
      "Trivago\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing data with matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt #refer to as plt\n",
    "import numpy as np\n",
    "from math import log\n",
    "#list comprehension gives the squares of each\n",
    "#will plot with a connecting line\n",
    "plt.plot([x*x for x in range(100)])\n",
    "plt.ylabel('x^2')\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "print(\"with two parameters\")\n",
    "#provide an x and y series\n",
    "#also uses plusses instead of line\n",
    "plt.plot([2*x for x in range(50)], [3*log(x+1) for x in range(50)],marker= 'o')\n",
    "plt.show() #show the plot/graph\n",
    "#plt.plot([x in the range], [y in the range], what the point will look like)\n",
    "plt.legend(\"example\")\n",
    "\n",
    "#test\n",
    "print(\"Hotel?\")\n",
    "\n",
    "\"\"\"\n",
    "Can do scatterplots, histograms, etc.\n",
    "\"\"\"\n",
    "print(\"Trivago\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sympy\n",
    "from sympy import init_printing, Symbol, integrate\n",
    "#can use things from module. ex:\n",
    "#sympy.cos() and sympy.pi\n",
    "\n",
    "#initialize\n",
    "sympy.init_printing(use_unicode=False, wrap_like = False)\n",
    "\n",
    "#in traditional languages, cos() is a function that would be evaluated numerically,\n",
    "#The cos that we have imported will be\n",
    "print(sympy.cos(3*sympy.pi/4))\n",
    "\n",
    "#will just print (cos(10)) bc it can't do actual cosign (?)\n",
    "print(sympy.cos(10))\n",
    "\n",
    "#best printing option on display uses laTex for formating\n",
    "\n",
    "\n",
    "x = Symbol('x')\n",
    "\n",
    "\n",
    "#compute the integral of the polynomial function symbolicaaly\n",
    "#with respect to x\n",
    "integrate(x**2 + x + 1)\n",
    "\n",
    "\n",
    "print(integrate(cos(x) + pi*sin(), x) #integrate cos(x) + pi*sin() with respect to x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make own module\n",
    "\n",
    "import random\n",
    "\n",
    "def random_choice(first, second) :\n",
    "    choice = randrange(0,2)\n",
    "    \n",
    "    #etc.\n",
    "    \n",
    "\"\"\"\n",
    "modules to check out\n",
    "    scrapy = webscraping\n",
    "    baeutifulsoup = xml and html parsin\n",
    "    numpy = math extension\n",
    "    matplotlib -plotting things\n",
    "    nltk = natural language tookkit\n",
    "    sympy = symbolic math\n",
    "\"\"\"\n",
    "#test\n",
    "print(\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python files\n",
    "    #you have some data...\n",
    "\"\"\"\n",
    "    We are going to go over basics of reading text based  file. I assume format of file \n",
    "    is a number n followed by n pairs of numbers each on their own line\n",
    "    \n",
    "    ex:\n",
    "    4\n",
    "    1 2.0\n",
    "    2 4.0\n",
    "    3 6.0\n",
    "    4 8.0S\n",
    "    \"\"\"\n",
    "    \n",
    "#Open file\n",
    "\n",
    "#name of file is data.txt and should be plaintext\n",
    "\n",
    "\n",
    "#w is write, r is read, a is append. Write deletes everything and rewrites so don't do that\n",
    "my_data_file = open(\"data.txt\", \"r\")\n",
    "\n",
    "#prints location not contents\n",
    "print(my_data_file)\n",
    "\n",
    "#make a list of pairs\n",
    "#start empty\n",
    "\n",
    "result = []\n",
    "\n",
    "token = my_data_file.readline()\n",
    "\n",
    "#convert line to integer value\n",
    "count = int(token)\n",
    "print(count)\n",
    "\n",
    "for i in range(0, count) :\n",
    "    #read each line\n",
    "    line = my_data_file.readline()\n",
    "    \n",
    "#split data...\n",
    "\n",
    "\n",
    "#when done close file\n",
    "my_data_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading until you hit end of file\n",
    "\n",
    "#we have list of words, separated by spaces, lines that sometimes have nothing, some have word, some have words\n",
    "\n",
    "data_file = open(\"words.txt\", \"r\")\n",
    "\n",
    "#result is list of words\n",
    "\n",
    "result = []\n",
    "\n",
    "#iterate using a for loop\n",
    "for line in data_file :\n",
    "    #add each word\n",
    "    for word in line.split() :\n",
    "        result.append(word)\n",
    "    data_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-money",
   "metadata": {},
   "source": [
    "#pickle - data serialization\n",
    "#dump/load(file)\n",
    "#\"we won't really use this\"- quote by Hoot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-queens",
   "metadata": {},
   "source": [
    "#regular expression are a powerful tool with several powerful tools, unfortunately they are a tool that people \n",
    "#like to misuse, which is the root cause of why they suck lmao\n",
    "\n",
    "#They are a regular search pattern, a special string that will either accept or reject another string. as Abstract\n",
    "#concepts, regular expressions can be messy.\n",
    "\n",
    "\n",
    "#RE model\n",
    "#\"We won't use it unless you need to do something weird with data in it\" -hoot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-description",
   "metadata": {},
   "source": [
    "#pandas and data frames\n",
    "\n",
    "#intro\n",
    "#number of places that have open data sets you can aquire\n",
    "\n",
    "#Check out the data-set notebook for list of places you can use\n",
    "#use govt databases\n",
    "    #WHO\n",
    "#Any data we work with is in CSV file format\n",
    "    #Excel CAN be saved as a CSV\n",
    "\n",
    "#Stands for comma separated values (gives you a good idea of what it is)\n",
    "\n",
    "\n",
    "#Format:\n",
    "\n",
    "\"\"\"\n",
    "First line: one header string for each data field\n",
    "Remaining lines of the file contains a single data record with values separated by commas\n",
    "any field MAY surround the value with double quotes. Some data requires the use of double quotes\n",
    "\"\"\"\n",
    "#uses .csv extension\n",
    "#spaces can cause issues with non-numeric fields\n",
    "#can have empty spaces but not recommended \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floral-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reflected-verification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alligator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>panda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>falcon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>parrot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      animal\n",
       "0  alligator\n",
       "1        bee\n",
       "2      panda\n",
       "3     falcon\n",
       "4       lion\n",
       "5      monke\n",
       "6     parrot"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic data frames quick look\n",
    "#keys() displays a list of the names of the fields\n",
    "#head() table display of the first few lines in the data frame (5 lines default, head(n=20) gives first 20)\n",
    "    #ex: dataFrame.head(n=20) \n",
    "#Get to know panda, matplotlib, dataFrame\n",
    "\n",
    "df = pd.DataFrame({\"animal\":[\"alligator\", \"bee\", \"panda\", \"falcon\", \"lion\", \"monke\", \"parrot\"]})\n",
    "df\n",
    "\n",
    "#use .head, .tail\n",
    "\n",
    "#use info() to look for missing values and type of each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For BBHW2\n",
    "\n",
    "sum = 0\n",
    "for i in range(1,11):\n",
    "    sum += (1/i**2)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cross-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Matrix: \n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Print ragged\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Print sin of ragged\n",
      "[[ 0.84147098  0.90929743  0.14112001]\n",
      " [-0.7568025  -0.95892427 -0.2794155 ]\n",
      " [ 0.6569866   0.98935825  0.41211849]]\n"
     ]
    }
   ],
   "source": [
    "#imports should be working now\n",
    "matrixA = np.matrix([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "\n",
    "ragged = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print(\"\\nOur Matrix: \")\n",
    "print(matrixA)\n",
    "print(\"\\nPrint ragged\")\n",
    "print(ragged)\n",
    "print(\"\\nPrint sin of ragged\")\n",
    "print(np.sin(ragged))"
   ]
  },
  {
   "attachments": {
    "22ffe88a-8364-4060-ab3a-24c062e37a17.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "informal-steps",
   "metadata": {},
   "source": [
    "Correlations coefficients:\n",
    "\n",
    "![image.png](attachment:22ffe88a-8364-4060-ab3a-24c062e37a17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "collectible-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='exampleCsv.csv' mode='r' encoding='cp1252'>\n",
      "<_io.TextIOWrapper name='exampleCsv.csv' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "#dataframes plots\n",
    "\n",
    "plot()\n",
    "    kind = scatter, line, hist\n",
    "    x\n",
    "    y\n",
    "    figsize = tuple that \n",
    "\n",
    "\n",
    "\n",
    "numpy scatter()\n",
    "    x = array like thing\n",
    "    y = array like thing\n",
    "\n",
    "\n",
    "#he is ZOOMING through these slides\n",
    "\n",
    "\"\"\"\n",
    "drop() and fill() are functions to help edit your data\n",
    "make a copy of data before you edit it so you don't lose original data\n",
    "\"\"\"\n",
    "\n",
    "#drop a row if all values in the subset are missing\n",
    "data_copy = open(\"exampleCsv.csv\", \"r\")\n",
    "\n",
    "for i in data_copy:\n",
    "    data_copy_2[i] = data_copy[i]\n",
    "    print(data_copy[i])\n",
    "    \n",
    "print(data_copy_2)\n",
    "#data_copy_2.dropna(axis='columns', how='any', inplace=True)\n",
    "print(data_copy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-amsterdam",
   "metadata": {},
   "source": [
    "Models\n",
    "start with set of instances\n",
    "    decide what model to use\n",
    "    for each model, decide on number of parameters the model will use\n",
    "    train each model on the training data\n",
    "    select the best model according to some performance measure(metric)\n",
    "This is a minimization problem, potential problem. For most models, as parameters are added. The model will do a better job fitting the training data. The problem is we will overfit the training data. Delicate balance between too many and too few parameters\n",
    "\n",
    "Solution to the problem is to take our initial data set and split it into two sets\n",
    "    the training set - use this to train model\n",
    "    the test set - use this to come up with performance of model\n",
    "    \n",
    "    \n",
    "    \n",
    "Overfit\n",
    "    not allowed to break out test data until we have finished creating our model. So, how do we decide if we are overfitting data?\n",
    "    Technique involves splitting the training set into smaller training sets and hold onto a \"validation set\". Use the training set to find the best parameters for the model and then grade the perfromance of the model on the validation set\n",
    "    \n",
    "    Classic overfitting\n",
    "        as I train model, I get better and better performance on the training set, but performance on validation set doesn't improve.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaptive-intensity",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-2-52dc871b300e>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-52dc871b300e>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    splitter = StratifiedShuffleSplit(n_splits = 1, test_size =0.2, random_state(123)):\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "#2/9/21 notes\n",
    "\n",
    "#Make a program that splits data into training sets and test sets\n",
    "import numpy as np\n",
    "def fractional_split(data_set, test_fraction=0.2,seed=42):\n",
    "    data_count = len(data_set)\n",
    "    test_count = int(test_fraction*data_count)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    shuffled_indices = np.random.permutation(data_count)\n",
    "    \n",
    "    #use the front of the shuffled list as test set\n",
    "    #use the back of the shuffled list as the training set\n",
    "    test_indices = shuffled_indices(test_count)\n",
    "    train_indices = shuffled_indices(test_count)\n",
    "    \n",
    "    return data_set.iloc[train_indeces], data_set.iloc[test_indeces]\n",
    "\n",
    "import pandas as pd\n",
    "#sep = \";\" can be \",\"\n",
    "data_frame = pd.read_csv(\"h.csv\", sep=\";\")\n",
    "#train_set, test_set =\n",
    "\n",
    "print(len())\n",
    "\n",
    "#scikit-learning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(data_frame, test_size=0.2,random_state=123)\n",
    "\n",
    "#print some information\n",
    "print(len(train_set), len(test_set))\n",
    "print(train_set.head())\n",
    "print(test_set.head())\n",
    "print(test_set[\"male\"].value_counts(0)) #In \"\" put the field head\n",
    "print(test_set[\"male\"].value_counts(1))\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits = 1, test_size =0.2, random_state(123)):\n",
    "    for train_indices, test_indices in splitter.split(data_frame, data_frame[\"m\"])\n",
    "    #The body only executes once vecause the number of splits is one\n",
    "    train_set = data_frame.iloc[train_indices]\n",
    "    test_set = data_frame.iloc[test_indices]\n",
    "print(train_set[\"male\"].value_counts())\n",
    "print(train_set[\"male\"].value_counts())\n",
    "\n",
    "\n",
    "data_frame[\"bmi\"] = 10000*data_frame[\"weight\"]/data_frame[\"height\"]**2)\n",
    "data_frame.head()\n",
    "data_frame.describe()\n",
    "\n",
    "#divide into evenly sized discrete buckets\n",
    "data_frame[\"bmi_category\"] = np.ceil(data_frame[\"bmi\"]/3)\n",
    "data_frame.head()\n",
    "print(data_frame[\"bmi category\"].value_counts())\n",
    "\n",
    "def bmi_rater(bmi):\n",
    "    if bmi <18.5 :return 1\n",
    "    if bmi <25 : return 2\n",
    "    if bmi <30 : return 3\n",
    "    return 4\n",
    "\n",
    "#map it onto the bmi\n",
    "data_frame[\"bmi_category\"] = data_frame[\"bmi\"].map(bmi_rater)\n",
    "data_frame.head\n",
    "print(data_frame[\"bmi_category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always work with a copy\n",
    "working_set = copy(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affected-richardson",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1446e40de9af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#split into male/female\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfemale_age\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_where\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"male\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmale_age\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_where\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"male\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#split into male/female\n",
    "\n",
    "female_age = np.ma.masked_where(data_frame[\"male\"]==1, data_frame[\"age\"])\n",
    "\n",
    "male_age = np.ma.masked_where(data_frame[\"male\"]==0, data_frame[\"age\"])\n",
    "\n",
    "\n",
    "plt.scatter(x=male_age, y=data_frame[\"bmi\"], marker=\"o\", c=\"green\")\n",
    "\n",
    "plt.scatter(x=female_age, y=data_frame[\"bmi\"], marker=\"o\", c=\"yellow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-fairy",
   "metadata": {},
   "source": [
    "#Statistics and hypothesis\n",
    "\n",
    "#make a guess\n",
    "\n",
    "#suppose that we have some data and we want to get some useful information out of it: Form a hypothesis\n",
    "\n",
    "#Get statistical evidence to support (or not support) your hypothesis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "graphic-evaluation",
   "metadata": {},
   "source": [
    "2/11/21 notes\n",
    "\n",
    "A fairness test\n",
    "We did the whole thing on coin flip statistics last time\n",
    "\n",
    "ANOVA\n",
    "suppose that we take two sets of data and we compute an average and standard deviation for both of them. Did they come from the same distribution?\n",
    "\n",
    "Do they have the \"same\" average?\n",
    "Do they have the \"same\" standard deviation\n",
    "\n",
    "ANOVA = Analysis of Variance gives a p score for the null hypothesis that the data came from the same distrubution. \n",
    "\n",
    "\n",
    "Questions:\n",
    "    Give an example of a statistical test\n",
    "    What is a null hypothesis?\n",
    "    What is a p-value? What ranges can it have?\n",
    "    Is it better to havea  p value close to 1 or close to 10?\n",
    "    \n",
    "H H T H T  T H T T T = mine 5 correct\n",
    "H H H T T  H T T H T = random\n",
    "\n",
    "\n",
    "Binary classification\n",
    "    In binary classification we will use data to create a model that has a single yes/no output.\n",
    "    In some cases we get a probability expressing our confidence in the result\n",
    "    \n",
    "    \n",
    "Make a performance model\n",
    "True positive TP: Predicted 1 actually 1\n",
    "False Positive FP: Predicted 1 acutally 0 (Type 1 error)\n",
    "True negative TN: Predicted 0 actually 0\n",
    "False negative FN: Predicted 0 actually 1 (Type 2 error)\n",
    "\n",
    "Accuracy:\n",
    "(TP + TN)/(TP + FP + TN + FN)\n",
    "\n",
    "Metric precision:\n",
    "TP/TP+FP\n",
    "Reduce false positives\n",
    "\n",
    "Metric sensitivity:\n",
    "TP/TP+FN\n",
    "Reduce false negatives\n",
    "\n",
    "Metric specificity:\n",
    "TN/TN+FP\n",
    "Reduce false ?\n",
    "\n",
    "Combined Metrics\n",
    "F1 score: Harmonic mean of the precision and sensitivity\n",
    "2/((1/precision)+(1/sensitivity))\n",
    "\n",
    "G score: Geometric mean of precision and sensitivity\n",
    "sqrt(precision*sensitivity)\n",
    "\n",
    "\n",
    "Scanning\n",
    "One way to approach this kind of problem is to order the instances and then set the threshhold between each neightboring pair of instances. Each will change the confisuion matrix and our measures. We can then choose the best\n",
    "    Highest accuracy\n",
    "    Highest F score\n",
    "    \n",
    "Log loss\n",
    "    consider following set of data, we can create a binary classifier for each that uses the same threshold. Both will have the same confusion matrix, but intuitievly one model is better\n",
    "        Data set 1: 3 TP, 1FP, 2TN, 1FN, nothing right on the line\n",
    "        Data set 2: same numbers, but the values are closer to the dividing line\n",
    "        \n",
    "To calculate log loss\n",
    "    Reward high confidence predictions and penalize high confidence predictions that are wrong\n",
    "    p - the models output probability that the instance is positive (0<p<1)\n",
    "        Close to 0 is a strong prediction that the instance is negative\n",
    "        Close to 0.5 is a weak presdiction\n",
    "        close to 1 is a strong prediction that the instance is positive\n",
    "    y = an indicator for the classification of the instance (0,1)\n",
    "    log-loss = -(y * ln(p) + (1+y)ln(1-p))\n",
    "        ln0 can't happen for ln(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-crash",
   "metadata": {},
   "source": [
    "#notes 2/18/2021\n",
    "\n",
    "Log loss\n",
    "\n",
    "Test your model:\n",
    "    Use model to compute log loss for each instance in a data set and then take the average\n",
    "    prefer a model that has an average log loss that is closer to zero\n",
    "\n",
    "Linear Predictor\n",
    "    In linear regression, we will use data to create a model that has a single numerical output\n",
    "    Training data feeds into the trainer, trainer and new data instance feed into model, model outputs value\n",
    "    \n",
    "Linear model\n",
    "    in a linear regression, sum the input values * a coefficient\n",
    "    Account for bias\n",
    "    There is a long ass equation on the board that I can't write in time.\n",
    "        Predicted value = bias + model parameters +feature values + modelparameters(number of values) + feature \n",
    "        values (number of values) where (number of values) is a subscript\n",
    "\n",
    "Linear model (vector form) featuring calculus (fuck)\n",
    "    in a linear regression, sum the input values * corefficient\n",
    "    bundle everything from above into a vector\n",
    "        x = [1, x, x(2), ..., x(x)] = just examples of data, x(0) = feature vector, set to 1.\n",
    "        ^ that model should be vertical btw\n",
    "    Parameter vector and transpose = [0(0), 0(1), 0(2), ... , 0(x)] ((#) = subscript\n",
    "    \n",
    "    Do matrix multiplication between them and that's your answer\n",
    "    \n",
    "    \n",
    "Evaluating the model\n",
    "    there are a number of functions we can use to measure the performance but here are a couple standard techniques\n",
    "    For each data instance, compute the distance between actual and predicted values\n",
    "        Absolute (L1 or Manhattan norm)\n",
    "        Square (L2 or Eucildean norm)\n",
    "        Higher Powers (Lk norm)\n",
    "    Average over all instances\n",
    "        norms = the lk norm of a vector v is defined as follows. The higher the exponenet, the more sensitive the \n",
    "        norm is to outliers\n",
    "            ||v||(k) = (|v(1) I^k + |v(2)|^k + ... |v(n)|^k)^I/k\n",
    "            ||v|| = distance measurement\n",
    "\n",
    "Evaluating the model\n",
    "    Root Mean Square Error (RMSE)\n",
    "    Use L2 norm and compute\n",
    "        big ass complicated exquation\n",
    "        x is training data and includes both x^i and y^i\n",
    "        h is hypothesis function (model)\n",
    "    \n",
    "Minimizing a function\n",
    "    Suppose that we want to find the minimum value of a function\n",
    "        'min' is relative, a curvy line can have multiple minimums and multiple maximums\n",
    "            Can be found by finding where the slope is zero\n",
    "            \n",
    "            \n",
    "Errors revisited\n",
    "    MSE: For linear regression, error function is well behaved and if we compute the gradient we get linear\n",
    "    equations that can be solved directly for the minimum, no search required\n",
    "    \n",
    "   RMSE: Error function that is well behaved, but the gradient edoes not result in linear equations, use search\n",
    "    \n",
    "   MAE: Error function is not well behaved (Abs value does not have a derivative everywhere) use search\n",
    "   \n",
    "   \n",
    "   \n",
    "Gradient Descent\n",
    "    When we perform a Grad Desc we have to be careful how big of a step to take\n",
    "        if we step too much we could jump over the minimum\n",
    "        When gradient/slope is zero, we have a critical point\n",
    "        \n",
    "        \n",
    "Finding gradient\n",
    "    compute partial derivative for each regression parameters\n",
    "        fuuuuuuck\n",
    "    There is a huge equation on the board that I can't write in time relating to partial derivatives\n",
    "        Error function is quadratic, derivative is linear (this is something you remember from calc :0)\n",
    "    Setting derivative of E to zero gives us two linear equations we can solve\n",
    "    \n",
    "   Solving these:\n",
    "       If we have a linear system we can express it using matrices and vectors\n",
    "       -                                - -  -   -  -\n",
    "       | a(1,1) a(1,2) a(1,3)...         || a| = |xyz|\n",
    "       | a(2, 1)...    .... \n",
    "       \n",
    "   those are matrices ^ [values ] * [values]\n",
    "   \n",
    "   Find the inverse matrix to A such that when multiplied you get an identity matrix\n",
    "       A^-1\n",
    "   multiply by inverse to solve for x\n",
    "       A^-1 *A*x = Ix = x = A^-1 b\n",
    "           Ix = identity matrix\n",
    "   Some matrices are degenerates that belong on a cross and don't have an inverse\n",
    "       If a matrix is close to being a degenerate it gets math errors\n",
    "Solving linear regression\n",
    "   Setting derivate of E to zero gives two linear equations with two unknowns\n",
    "       yuuuuuge equation\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verified-customer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c61608ca11c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlin_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#2 Fit the model to data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#arraylike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#vectorlike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlin_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "#SKlearn\n",
    "#1 create a model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "#2 Fit the model to data\n",
    "x = train_set[[\"feature\"]] #arraylike\n",
    "y = train_set[\"target\"] #vectorlike\n",
    "lin_reg.fit(x, y)\n",
    "#3 Score the model\n",
    "r2 = lin_reg.score(x,y)\n",
    "#4 Evaluate the model with the test data find the MSE and R2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "x_eval = test_set[[\"feature\"]]\n",
    "y_true = test_set[\"target\"]\n",
    "y_pred = line_reg.score(x_eval, y_true)\n",
    "rms_error = mean_squared_error(y_true, y_pred)\n",
    "#5 Use the model to make predictions with new data\n",
    "x_predict = [[17], [20], [19]] #3 instances\n",
    "y_predict = lin_reg.predict(x_predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "seasonal-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "2\n",
      "3  ,  1  ,  4\n",
      "2  ,  1  ,  4\n",
      "1  ,  1  ,  4\n",
      "[3, 2, 1, 2]\n",
      "2 is a duplicate\n",
      "2 is a duplicate\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "def next(x=123, b=29, m=47):\n",
    "    return ((x**2)+b)%m\n",
    "\n",
    "print(next())\n",
    "\n",
    "#start = int(input(\"Enter an integer value\"))\n",
    "#b = int(input(\"Enter another integer value\"))\n",
    "#m = int(input(\"Enter another integer value\"))\n",
    "\n",
    "print(next(start, b, m))\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "(Python) Use the next function to create a list of numbers where the first value in the list\n",
    "is start and to find the next value in the list, apply the next function to previous value as x. \n",
    "Stop when you have m values in the list.\n",
    "So if start=3, b=1, and m= 4, the list would be [ 3, 2, 1, 2].  (The value after 3 is 2 because \n",
    "next( 3, 1, 4) is (3^2+1 )mod_4=2.  The value after 2 is next( 2, 1, 4) is (2^2+1 )mod_4=1.   \n",
    "The value after 1 is next( 1, 1, 4) is (1^2+1 )mod_4=2. Stop because the number of values is 4.\n",
    "\"\"\"\n",
    "start = 3\n",
    "b = 1\n",
    "m = 4\n",
    "my_list = [start]\n",
    "\n",
    "for i in range(m-1):\n",
    "    my_list.append(next(my_list[i], b, m))\n",
    "    print(my_list[i], \" , \", b, \" , \", m)\n",
    "    \n",
    "print(my_list)\n",
    "\n",
    "\n",
    "for i in my_list:\n",
    "    if my_list.count(my_list[i]) > 1:\n",
    "        print(my_list[i], \"is a duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "breathing-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 2]\n",
      "2 is a duplicate\n",
      "2 is a duplicate\n"
     ]
    }
   ],
   "source": [
    "start = 3\n",
    "b = 1\n",
    "m = 4\n",
    "my_list = [start]\n",
    "\n",
    "for i in range(m-1):\n",
    "    my_list.append(next(my_list[i], b, m))\n",
    "print(my_list)\n",
    "\n",
    "\n",
    "for i in (my_list):\n",
    "    if my_list.count(my_list[i]) > 1:\n",
    "        print(my_list[i], \"is a duplicate\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "chronic-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for BBHW5 Bonus 2\n",
    "\n",
    "\n",
    "input = open(\"primes.txt\", \"r\")\n",
    "count = 0\n",
    "\n",
    "#def split(word):\n",
    "#   return list(word)\n",
    "\n",
    "for i in input:\n",
    "    list = []\n",
    "    list[:] = i\n",
    "    if list.count(\"\\n\")>0:\n",
    "        list.remove(\"\\n\")\n",
    "    for j in list:\n",
    "        count += int(j)\n",
    "    if count==9:\n",
    "        print(count, i, \"HERE??\")\n",
    "    count = 0\n",
    "        \n",
    "   \n",
    "   # if count==9 and count%9==0:\n",
    "    #    print(count, i, \"BOTH\")\n",
    "    if count==9 and count%9!=0:\n",
    "        print(count, i, \"HEREHEREHERE\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "criminal-ontario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 18, 27, 36, 45, 54, 63, 72, 81, 90, 108, 117, 126, 135, 144, 153, 162, 171, 180, 207, 216, 225, 234, 243, 252, 261, 270, 306, 315, 324, 333, 342, 351, 360, 405, 414, 423, 432, 441, 450, 504, 513, 522, 531, 540, 603, 612, 621, 630, 702, 711, 720, 801, 810, 900, 1008, 1017, 1026, 1035, 1044, 1053, 1062, 1071, 1080, 1107, 1116, 1125, 1134, 1143, 1152, 1161, 1170, 1206, 1215, 1224, 1233, 1242, 1251, 1260, 1305, 1314, 1323, 1332, 1341, 1350, 1404, 1413, 1422, 1431, 1440, 1503, 1512, 1521, 1530, 1602, 1611, 1620, 1701, 1710, 1800, 2007, 2016, 2025, 2034, 2043, 2052, 2061, 2070, 2106, 2115, 2124, 2133, 2142, 2151, 2160, 2205, 2214, 2223, 2232, 2241, 2250, 2304, 2313, 2322, 2331, 2340, 2403, 2412, 2421, 2430, 2502, 2511, 2520, 2601, 2610, 2700, 3006, 3015, 3024, 3033, 3042, 3051, 3060, 3105, 3114, 3123, 3132, 3141, 3150, 3204, 3213, 3222, 3231, 3240, 3303, 3312, 3321, 3330, 3402, 3411, 3420, 3501, 3510, 3600, 4005, 4014, 4023, 4032, 4041, 4050, 4104, 4113, 4122, 4131, 4140, 4203, 4212, 4221, 4230, 4302, 4311, 4320, 4401, 4410, 4500, 5004, 5013, 5022, 5031, 5040, 5103, 5112, 5121, 5130, 5202, 5211, 5220, 5301, 5310, 5400, 6003, 6012, 6021, 6030, 6102, 6111, 6120, 6201, 6210, 6300, 7002, 7011, 7020, 7101, 7110, 7200, 8001, 8010, 8100, 9000, 10008, 10017, 10026, 10035, 10044, 10053, 10062, 10071, 10080, 10107, 10116, 10125, 10134, 10143, 10152, 10161, 10170, 10206, 10215, 10224, 10233, 10242, 10251, 10260, 10305, 10314, 10323, 10332, 10341, 10350, 10404, 10413, 10422, 10431, 10440, 10503, 10512, 10521, 10530, 10602, 10611, 10620, 10701, 10710, 10800, 11007, 11016, 11025, 11034, 11043, 11052, 11061, 11070, 11106, 11115, 11124, 11133, 11142, 11151, 11160, 11205, 11214, 11223, 11232, 11241, 11250, 11304, 11313, 11322, 11331, 11340, 11403, 11412, 11421, 11430, 11502, 11511, 11520, 11601, 11610, 11700, 12006, 12015, 12024, 12033, 12042, 12051, 12060, 12105, 12114, 12123, 12132, 12141, 12150, 12204, 12213, 12222, 12231, 12240, 12303, 12312, 12321, 12330, 12402, 12411, 12420, 12501, 12510, 12600, 13005, 13014, 13023, 13032, 13041, 13050, 13104, 13113, 13122, 13131, 13140, 13203, 13212, 13221, 13230, 13302, 13311, 13320, 13401, 13410, 13500, 14004, 14013, 14022, 14031, 14040, 14103, 14112, 14121, 14130, 14202, 14211, 14220, 14301, 14310, 14400, 15003, 15012, 15021, 15030, 15102, 15111, 15120, 15201, 15210, 15300, 16002, 16011, 16020, 16101, 16110, 16200, 17001, 17010, 17100, 18000, 20007, 20016, 20025, 20034, 20043, 20052, 20061, 20070, 20106, 20115, 20124, 20133, 20142, 20151, 20160, 20205, 20214, 20223, 20232, 20241, 20250, 20304, 20313, 20322, 20331, 20340, 20403, 20412, 20421, 20430, 20502, 20511, 20520, 20601, 20610, 20700, 21006, 21015, 21024, 21033, 21042, 21051, 21060, 21105, 21114, 21123, 21132, 21141, 21150, 21204, 21213, 21222, 21231, 21240, 21303, 21312, 21321, 21330, 21402, 21411, 21420, 21501, 21510, 21600, 22005, 22014, 22023, 22032, 22041, 22050, 22104, 22113, 22122, 22131, 22140, 22203, 22212, 22221, 22230, 22302, 22311, 22320, 22401, 22410, 22500, 23004, 23013, 23022, 23031, 23040, 23103, 23112, 23121, 23130, 23202, 23211, 23220, 23301, 23310, 23400, 24003, 24012, 24021, 24030, 24102, 24111, 24120, 24201, 24210, 24300, 25002, 25011, 25020, 25101, 25110, 25200, 26001, 26010, 26100, 27000, 30006, 30015, 30024, 30033, 30042, 30051, 30060, 30105, 30114, 30123, 30132, 30141, 30150, 30204, 30213, 30222, 30231, 30240, 30303, 30312, 30321, 30330, 30402, 30411, 30420, 30501, 30510, 30600, 31005, 31014, 31023, 31032, 31041, 31050, 31104, 31113, 31122, 31131, 31140, 31203, 31212, 31221, 31230, 31302, 31311, 31320, 31401, 31410, 31500, 32004, 32013, 32022, 32031, 32040, 32103, 32112, 32121, 32130, 32202, 32211, 32220, 32301, 32310, 32400, 33003, 33012, 33021, 33030, 33102, 33111, 33120, 33201, 33210, 33300, 34002, 34011, 34020, 34101, 34110, 34200, 35001, 35010, 35100, 36000, 40005, 40014, 40023, 40032, 40041, 40050, 40104, 40113, 40122, 40131, 40140, 40203, 40212, 40221, 40230, 40302, 40311, 40320, 40401, 40410, 40500, 41004, 41013, 41022, 41031, 41040, 41103, 41112, 41121, 41130, 41202, 41211, 41220, 41301, 41310, 41400, 42003, 42012, 42021, 42030, 42102, 42111, 42120, 42201, 42210, 42300, 43002, 43011, 43020, 43101, 43110, 43200, 44001, 44010, 44100, 45000, 50004, 50013, 50022, 50031, 50040, 50103, 50112, 50121, 50130, 50202, 50211, 50220, 50301, 50310, 50400, 51003, 51012, 51021, 51030, 51102, 51111, 51120, 51201, 51210, 51300, 52002, 52011, 52020, 52101, 52110, 52200, 53001, 53010, 53100, 54000, 60003, 60012, 60021, 60030, 60102, 60111, 60120, 60201, 60210, 60300, 61002, 61011, 61020, 61101, 61110, 61200, 62001, 62010, 62100, 63000, 70002, 70011, 70020, 70101, 70110, 70200, 71001, 71010, 71100, 72000, 80001, 80010, 80100, 81000, 90000]\n"
     ]
    }
   ],
   "source": [
    "#for i in range(9999):\n",
    "#   print(i, end= \" \")\n",
    "    \n",
    "count = 0\n",
    "\n",
    "#def split(word):\n",
    "#   return list(word)\n",
    "\n",
    "resultList = []\n",
    "for i in range(99999):\n",
    "    #split number into a list\n",
    "    list = []\n",
    "    list[:] = str(i)\n",
    "    \n",
    "    #iterate\n",
    "    for j in list:\n",
    "        count += int(j)\n",
    "    #reset counter\n",
    "     \n",
    "    if(count==9):\n",
    "        resultList.append(i)\n",
    "    count = 0\n",
    "    \n",
    "resultList.sort()\n",
    "print(resultList)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "burning-auction",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-9df670d63077>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-9df670d63077>\"\u001b[1;36m, line \u001b[1;32m52\u001b[0m\n\u001b[1;33m    x_data = min_val +\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#2/25/2021 notes\n",
    "\n",
    "#Solving the linear regression\n",
    "\n",
    "\"\"\"\n",
    "Setting the derivative of E to zero gives two linear equations in two unknowns\n",
    "lots of equations on screen.\n",
    "    Tons of symbols no way in hell I type\n",
    "    \n",
    "Performance:\n",
    "    This performs well with larger training sets and scales linearly with the number of predictions to be made\n",
    "\n",
    "In sklearn\n",
    "    create a model\n",
    "    I have this code above\n",
    "    \n",
    "    \n",
    "R2 - coefficient of determination\n",
    "    The coefficient of determination tells us what percentage of the variation in our\n",
    "    output y is determined by the variation in the input x. The maximum value it can \n",
    "    have is 1. It can be arbitrarily bad (negative)\n",
    "R^2 = 1- ((ss(res)/ss(tot)))\n",
    "        ss residual / ss total\n",
    "        Uh I can't see the board... yikes\n",
    "        \n",
    "Root mean square error\n",
    "    Roughly this is a standard deviation\n",
    "    For Gaussian Distrubution we expect:\n",
    "        1) 68% Of the value is within 1 standard deviations\n",
    "        2) 95% Of the value is within 2 standard deviations\n",
    "        3) 99.7% Of the values are within 3 standard deviations\n",
    "\"\"\"\n",
    "\n",
    "#Example code:\n",
    "\n",
    "print(\"The bias is: \", reg.intercept_)\n",
    "print(\"The feature coefficients are \", reg.coef_)\n",
    "#_ at the end is something something example\n",
    "#Gave us an entire presentation about how sucky his linear line was for reviewing his height/weight example.\n",
    "#His model was off +-50lbs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Polynomial Regression\n",
    "    Instead of using a linear predictor, we can use any function to do our curve fitting. Ex: cubic\n",
    "    \n",
    "    \n",
    "Use a quadratic to make a thing\n",
    "\"\"\"\n",
    "#Review slides, polynomial regression towards end of class (20 mins till end)\n",
    "#This will generate an array of values from min to max\n",
    "x_data = min_val + \n",
    "\n",
    "\n",
    "\n",
    "#Try fitting a line\n",
    "from sklearn_ ...\n",
    "\n",
    "#Try fitting with a quadratic\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "create_poly_terms = PolynomialFeatures(degree = 2, include_bias=...)\n",
    "x_poly_terms ...\n",
    "\n",
    "#This bitch is FLYING through slides like bro you already have it typed\n",
    "\n",
    "\n",
    "from sklearn.linear_model import linearRegression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(x_poly_terms, y_data)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "poly_model = ElasticNet(alpha = 20, li_ratio=0.5, max_iter = 150)\n",
    "\n",
    "#Overfitting tends to be very wiggly, it is a jank-*ss line that misses widly but hit's /exactly/ some things.\n",
    "#Almost looks like a barcode in some spots but a flatline in others\n",
    "\n",
    "#For next time: Batch Gradient Descent\n",
    "\n",
    "\"\"\"\n",
    "Batch Gradient Descent\n",
    "\n",
    "Computer the gradient using the entire training set at each iterations.\n",
    "If the cost function is well behaved, and the learning rate is not too high, we will\n",
    "eventually (though possibly slowly) converge to a minimum\n",
    "\n",
    "\n",
    "Stochastic gradient descent\n",
    "pick one random data instance at each iteration and use it to estimate the gradient\n",
    "advantages:\n",
    "    fast\n",
    "    can work with very large data sets since entire set doesn't need to be in memeory\n",
    "    less likely to get trapped in a local minimum\n",
    "disadvantages\n",
    "    does not progress at a steady rate to a minimum, but bounches around\n",
    "    does not take advantage of vector hardware like gpus\n",
    "minibatch\n",
    "    is stochastic, exccept we take more than once instance at a time, though not the entire data set. \n",
    "    If has performance that is in between batch and stochastic\n",
    "    \n",
    "    \n",
    "Cross validation\n",
    "    if we use different data, linear regression will come up with a different model (varies the parameters). \n",
    "    How do we know if our model is good for the general population?\n",
    "    1. Create a holdout validation set from the training set and use that as the metric of how well our model is doing\n",
    "    2. Randomly split the data set into K pieces (folds)\n",
    "        a. Run (train) the model k times\n",
    "        b. Each run uses 1 of the folds as the validation set\n",
    "        c. The training set is the other k-1 folds\n",
    "        d. Measure the performance of the model on the validation set\n",
    "        e. Combine each of the k scores into a single metric\n",
    "    3. Capture the model with the best performance\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Code for cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin reg= LinearRegression()\n",
    "\n",
    "x= working_set[[\"InputFeature\", \"InputFeature2\"]]\n",
    "y= working_set[\"TargetFeature\"]\n",
    "strat= workin_set[\"StratifierFeature\"]\n",
    "\n",
    "spliter = StratifiedKFold(n_splits = 10, random_state =10) #10 folds\n",
    "results = []\n",
    "\n",
    "for train_indices, validate_indices in spliter.split(x, strat):\n",
    "    fold_lin_reg = copy(lin_reg)\n",
    "    X_train = x.iloc(train_indices)\n",
    "    y_train = y.iloc(train_indices)\n",
    "    fold_lin_reg.fit(x_train,y_train)\n",
    "    \n",
    "    x_validate = x.iloc(validate_indicies)\n",
    "    y_validate = y.iloc(validate_indicies)\n",
    "    y_predicted = fold_lin_reg.predict(x_validate)\n",
    "    fold_score = mean_squared_error(y_validate,y_predicted)\n",
    "    results.add(fold_score)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Model error\n",
    "Error in model comes from 3 sources:\n",
    "\n",
    "Bias-error due to an incorrect assumption about the nature of the data. A high bias model is likely to underfit\n",
    "the training data\n",
    "\n",
    "Variance-error due to a model's sensitivity to small variations in the training data. A model with a high\n",
    "number of degrees of freedom (parameters) is likely to have a high variance and overfit the training data\n",
    "\n",
    "irreducible-error that is associated with noise in the data.\n",
    "\n",
    "\n",
    "Bias vs Variance\n",
    "If we increase model's complexity, we are likely to decrease the bias and increase the variance and vice-versa\n",
    "\n",
    "To deal with reeducible error, we need to...\n",
    "\n",
    "Diagnostic: Training error is much smaller than test error (overfitting) = Variance problem\n",
    "\n",
    "Diagnostic: Training error is high (underfitting) = Bias problem\n",
    "\n",
    "\n",
    "Regularized Linear Models\n",
    "\n",
    "We know that overfitting is a potential problem, one way to handle the overfitting is to constrain (regularize)\n",
    "the model. There are a number of such techniques (Ridge, lasso, elastic net, early stopping)\n",
    "\n",
    "\n",
    "Ridge Regression (Tikhonov)\n",
    "\n",
    "cost(theta) = MSE(theta) + q((a*.5)summation i=1, n, theta * 2/i)\n",
    "\n",
    "Has a closed form solution (*Symbols*)\n",
    "Where A is the identity matrix with a zero in the upper left instead of a 1\n",
    "\n",
    "Alpha is a hyper-parameter\n",
    "\n",
    "if alpha is zero, we have our standard linear regression\n",
    "The larger the alpha is, the more the weights end up being close to zero and we get a line \n",
    "going through the mean of the data\n",
    "Ridge regression...\n",
    "\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and selection operator)\n",
    "No closed form solution... use a gradient descent technique. Scale before applciation\n",
    "This model tends to make features that are uimportant to the fit zero.\n",
    "\n",
    "\n",
    "Elastic Net Regression\n",
    "This is a mix of ridge and lasso. Hyperparameter r determines the ratio of each.\n",
    "No closed form solution. Use a gradient descent technique. Scale before application\n",
    "\n",
    "Early stopping\n",
    "On gradient descent, you keep taking steps as long as the error is decreasing. With early stopping,\n",
    "you keep taking steps as long as the validation error is decreasing. Once it starts increasing, you are\n",
    "starting to overfit the data.\n",
    "The validation error may not be smooth, so you may stop if the validation error has not decreased in the last K\n",
    "iterations\n",
    "\n",
    "\n",
    "Which ones should we use?\n",
    "Ridge is good default\n",
    "Elastic or lasso if you suspect only a small number of features are contributing\n",
    "Lasso can behave erratically if the number of features is close to the number of training instances or \n",
    "there are features that are strongly correlated\n",
    "\"\"\"\n",
    "\n",
    "#Try fitting with a degree k polynomial\n",
    "\n",
    "k = 20\n",
    "\n",
    "create_poly_terms = PolynomialFeatures(degree=k, include_bias=false)\n",
    "x_poly_terms = create_poly_terms.fit_transform(x_data)\n",
    "\n",
    "#Poly_model = linear_regression\n",
    "\n",
    "import from sklearn.linear_model ElasticNet\n",
    "\n",
    "poly_model = ElasticNet(alpha=10, l1_ration=0.5,max_iter = 15000, normalize=True)\n",
    "\n",
    "poly_model.fit(x_poly_terms), y_data)\n",
    "\n",
    "intercept=poly_model.intercept_[0]\n",
    "coefs = poly_model.coef_[0]\n",
    "print(intercept, coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "roman-institute",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' on line 106 (<ipython-input-20-b000a331e986>, line 108)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-b000a331e986>\"\u001b[1;36m, line \u001b[1;32m108\u001b[0m\n\u001b[1;33m    ('scale_transform'), StandardSclaer())\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '[' on line 106\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SKlearn pipelines\n",
    "\n",
    "PipeLine:\n",
    "    A sequence of data processing components. The use of pipelines is common in machine learning\n",
    "    since there are often many transformations that need tobe applied to large amounts of data\n",
    "    \n",
    "    Components in a pipeline typically run asychronously. Components pull in data, process it, \n",
    "    and then pass it to the next component in the line. Data store mediates the process\n",
    "    \n",
    "    These kinds of systems are easy to understand\n",
    "        If a component breaks, pipeline can continue to work on old data\n",
    "        If a component breaks, it may be a while before the break is detected\n",
    "    \n",
    "Components:\n",
    "    Estimators - any model that can use data to estimate values\n",
    "    \n",
    "    Transformers - some estimators can be a transformation on a data set\n",
    "    \n",
    "    Predictors - some estimators can predict values\n",
    "    \n",
    "    Internal parameters of the estimators can be accessed through public attributes with an underscore suffix\n",
    "        ex: LinearRegression has bias_ and coef_\n",
    "\n",
    "\n",
    "Estimators:\n",
    "    The basic method that an estimator supports is:\n",
    "        fit(data_set, targets) - train the estimator on the data set to minimize some metric for the\n",
    "        desired output targets (labels)\n",
    "    \n",
    "Transformers\n",
    "    The basic methods they use:\n",
    "        fit()\n",
    "            uses arguments to decide how to perform the transformation\n",
    "        transform(data_set)\n",
    "            perform a given transformation on the data set and return the transformed data set. If the transformer is\n",
    "            in a pipeline, the transformed set is passed to the next object in the pipeline\n",
    "        fit_transform(data_set)\n",
    "            A convenience method that is equivalent to calling fit() and then transform() but sometimes is faster\n",
    "        \n",
    "        examples:\n",
    "            removing instances that do not meet some criteria or have missing values\n",
    "            replacing data values that are problematic or missing\n",
    "            adding new features\n",
    "            \n",
    "            \n",
    "Imputer\n",
    "    an imputer is a transformer that allows you to replace a missing data value in a data instance\n",
    "        SimpleImuter has available strategies:\n",
    "            'mean', 'median', 'most_frequent', 'constant'\n",
    "            \n",
    "            \n",
    "One-hot encoding\n",
    "    We have feature with discrete set of values.. Some learning algs attrubute meaning to numerical values.\n",
    "    ex: 0=Shoes, 1=groceries, 2=clothing\n",
    "        According to some learning algs shoe would be close to grocery but not to clothing. We can add one feature\n",
    "        for each of the possible values. For instance one feature will be hot(1) and the rest will be cold(0)\n",
    "        \n",
    "        Encoder expects an array, feature is vector, put it in the right form with reshape. Result is one\n",
    "        feature for each of the possible data values\n",
    "    \n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "data_store_cat_one_hot = encoder.fit_transform(data_set(\"store\").reshape(-1,1))\n",
    "\"\"\"\n",
    "Regularizer\n",
    "    kind of component that will make features have similar scales(value ranges). Some models (like regression) may\n",
    "    converge slowly\n",
    "    \n",
    "    Examples:\n",
    "        Standard scaler - convert to number of standard deviations away from the mean (centered on mean)\n",
    "        Robust scaler - robust to outliers (centered on median)\n",
    "        MaxAbsScaler(spelling?) - scale to maximum absolute value\n",
    "        MinMaxScaler - scale to a given range\n",
    "        Normalizere - make the L2 norm of every instance vector equal to 1\n",
    "        \n",
    "        can use PCA to decide which features are the most important\n",
    "        \n",
    "        \n",
    "Predictor\n",
    "    basic method that a predictor supports is \n",
    "        predict(data_set) - given a data set it will return a data set of predictions\n",
    "        predict_proba(data_set) - Give probabilities for the prediction using teh instances in the data set\n",
    "        score(data_set) - give a final score for the data set\n",
    "        \n",
    "        \n",
    "Building up a pipeline\n",
    "    sequence of components\n",
    "    last thing in pipeline must be an estimator\n",
    "    \n",
    "    each of the other items will be transformations. When you apply a pipeline to a data set and a target a\n",
    "    fit_transform ...\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "#Example:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LienarRegression\n",
    "\n",
    "howell_pipe = Pipeline([\n",
    "    ('median_transform', SimpleImputer()) #Add things from slides\n",
    "    ('scale_transform'), StandardSclaer())\n",
    "    ('lin_reg'), LinearRegression()\n",
    "    ])\n",
    "    \n",
    "print howell_pipe.named_steps('lin_reg')\n",
    "print(\"The bias is \",\n",
    "         howell_pipe.named_steps('lin_reg').intercept_)\n",
    "print(\"The feature coefficients are \",\n",
    "         howell)pipe.named_steps('lin_reg').coef_)\n",
    "\n",
    "\n",
    "\n",
    "#More complicated example\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn. #There is a yuuge example in the slides\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Decision trees\n",
    "    versatile machine learning algorithms\n",
    "        can do classification and regression\n",
    "        fundamental component of a random forrest\n",
    "        results are rules that can be understood\n",
    "        \n",
    "Tree questions example:\n",
    "\n",
    "Hoot is thinking of an animal\n",
    "Q1: Is it a mammal\n",
    "    y\n",
    "Q2: Has exoskeleton\n",
    "    n\n",
    "Q3: Does it have a tail\n",
    "    y\n",
    "Q4: As large or larger than 150lbs\n",
    "    y\n",
    "Q5: Does it fly\n",
    "    n\n",
    "Q6: Does it have legs\n",
    "    n\n",
    "Q7: Is it nocturnal\n",
    "    n\n",
    "Q8: Is it domesticated\n",
    "    n\n",
    "Q9: Is it a carnivour\n",
    "    y but unsure\n",
    "Q10: Does it have horns\n",
    "    n\n",
    "Q11: Does it swim/live in water?\n",
    "    y\n",
    "Q12: Does it live on land\n",
    "    n\n",
    "    \n",
    "Humpback whale\n",
    "\n",
    "\n",
    "\n",
    "Tree kinds\n",
    "    CART (Classification and regression trees) use GINI impurity to decide where to split\n",
    "    ID3 (Information Dichotomizer 3) use entropy and information gain\n",
    "    \n",
    "    Start with root node that has all the training instances, pick a feature to split on that gives the \"best\"\n",
    "    results. Continue until there are no more gains to be made or you hit a predetermined limit\n",
    "    Prefer splits where the children nodes are as pure as possible\n",
    "    \n",
    "    Trees are liable to overfit\n",
    "    \n",
    "GINI impurity\n",
    "    measure of the purity of a set\n",
    "    Ranges from 0 to 1 (0 is better)\n",
    "    \n",
    "Entropy\n",
    "    measure of how ordered a set of values is\n",
    "    Maximum order = set is pure, entropy is zero\n",
    "    Minimum order = set is equally mixed, some positive value\n",
    "    *Equation*\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suspected-waterproof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CSVClassifier' from 'sklearn.linear_model' (c:\\users\\s535982\\ds-venv\\lib\\site-packages\\sklearn\\linear_model\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-932a0de3931a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m#Make classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCSVClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0miris_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"petal length (cm)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"petal width (cm)\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CSVClassifier' from 'sklearn.linear_model' (c:\\users\\s535982\\ds-venv\\lib\\site-packages\\sklearn\\linear_model\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#Start 3/9/2021 notes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Splitting\n",
    "    Choice1: L=[20, 20, 0] R = [30, 30, 50]\n",
    "    Choice2: L=[10, 20, 40] R = [40, 30, 10]\n",
    "    \n",
    "    Which do we choose?\n",
    "    GINI for root node is 0.67\n",
    "    \n",
    "    Choice1: GINI([20, 20, 0]) = 1-[(20/40)^2 + (20/40)^2 + (0/40)^2] ~= 5\n",
    "            GINI ([30,30,50]) = 1-[(30/110)^2 + (30/110)^2 + (50/110)^2] ~=.64\n",
    "            WeightedAverage = (40/150)(0.5) + (110/150)(0.64) ~=.61\n",
    "    Choice2: GINI([10,20,40]) = 1-[(10/70)^2 + (20/70)^2 + (40/70)^2] ~=.57\n",
    "        GINI([40, 30, 10]) = 1-[(40/80)^2...]\n",
    "        WeightedAverage = \n",
    "        \n",
    "    \n",
    "Performance\n",
    "    Entropy and GINI typically give similar decision trees. In some cases, entropy will give better results\n",
    "    \n",
    "    Because entropy requires the computation of log, it is slower to computer than GINI\n",
    "    \n",
    "    The algorithm that creates/prunes the tree can have a larger effect.\n",
    "    \n",
    "    Note decision trees tend to overfit\n",
    "    \n",
    "The Iris Dataset\n",
    "    well known dataset that is a classifcation problem. It is used for testing the characteristics of neww machine learning algorithms\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris_data = load_iris()\n",
    "print(iris_data.keys())\n",
    "\n",
    "dict_keys=['data', 'target', 'target_names', 'DESCR', 'feature_names']\n",
    "\n",
    "\n",
    "#we can finesse it into a pandas dataframe\n",
    "\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "\n",
    "#add on the target\n",
    "iris_df['target'] = iris_data['target']\n",
    "    \n",
    "\"\"\"\n",
    "Image: The dataset has 4 measurable values. Sepal length/width and petal length/width. I encourage you to look at \n",
    "scatter plots with the other parameters, but we will look with petal length and width. Think about what would make them better or worse\n",
    "for use in a classifier.\n",
    "\n",
    "Imagine of scatterplot is a mostly linear thing with red scatter in the bottom left, then a blue line in the middle and a green \n",
    "line in the top right kind of resembling an y=4/3x kind of thing\n",
    "\"\"\"\n",
    "\n",
    "#Make classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SvmClassifier\n",
    "\n",
    "x= iris_df[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = iris_df['target']\n",
    "\n",
    "svm_classifer = SVC(kernel='linear')\n",
    "svm_classifier.fit(x, y)\n",
    "\n",
    "\n",
    "print(svm_classifier.support_) #the indices of the support vectors\n",
    "print(svm_classifier.support_vectors_) #The instances\n",
    "print(svm_classifier.n_support_) #number of support vectors for each\n",
    "     \n",
    "\"\"\"\n",
    "Measuring the fit\n",
    "        we have the full set of metrics that we can use\n",
    "    When we run the classifier on the iris data set, if only gets one instance wrong. You should be suspicious \n",
    "    that the tree is overfitting data.\n",
    "    \n",
    "    Before training the model, set aside a test set\n",
    "    \n",
    "    Do a k-fold cross validation\n",
    "    \n",
    "Getting the tree\n",
    "    we can greate a graphical representation of the tree we found\n",
    "    \n",
    "\"\"\"\n",
    "#export_graphiz... Mans is skipping everything of importance\n",
    "\n",
    "\n",
    "#Cross validate the model\n",
    "from sklearn.model_selection import KFold\n",
    "validation_accuracy = []\n",
    "validation_f1 = []\n",
    "#fold_and_validate = KFold(n_splits=5, shuffle =True, random_state=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction to SVM's (Support vector machines)\n",
    "\n",
    "\"\"\"\n",
    "King of the hill for classification, used to be. While neural nets have improved their performance and have overtaken SVM's, we still use them\n",
    "\n",
    "Pre 1980's: Most learning methods learn linear decision surfaces. Good theoretical understanding\n",
    "\n",
    "1980's: Decision trees and neural nets can learn non-linear decision surfaces. Theoretical basis is lacking, all methods have problems with local minimums\n",
    "\n",
    "1990's Non linear methods with a good theoretical underpinning are developed\n",
    "\n",
    "SVM: Use kernals to map non-linear decision durfaces to linear problems\n",
    "\n",
    "Neural nets - use of different activation functions and better initialization allow deep neural nets to learn complicated decision surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regulation-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 0 1]\n",
      " [1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "#Testing for homework 1 with confusion matrices\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cheap-oasis",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fdbd8f02b17e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Making the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "#Making the classifier\n",
    "from sklearn.svm import SVC\n",
    "x = iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southern-population",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3e768e22f4cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mdata_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_cluster_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m#he is skipping lots of code because he has it written down and doesn't GAF if students do\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "#3/16/2021\n",
    "\n",
    "\"\"\"\n",
    "SVM kernals\n",
    "K-means algorithm\n",
    "\n",
    "given n instances with m features, output k cluster centroids (k is a hyperparameter)\n",
    "\n",
    "1. Initialize the centroids (vector with m values) randomly\n",
    "2. Repeat until convergence\n",
    "    a. compute the distance from each traning instance to each centroid\n",
    "    b. assign each instance to the closest centroid\n",
    "    c. recompute the position of each centroid using the points that are assigned to it\n",
    "    \n",
    "    \n",
    "Convergence\n",
    "    define a distortion function j that is the sum of squared distances from each point to its assigned centroid\n",
    "        we are guarenteed that the value of J will decrease as we iterate\n",
    "        we may get trapped in a local minimum. Usually it isn't too bad but you can always rerun the algorithm with new random starting points.\n",
    "            choose the one that gives the minimum value of j\n",
    "        while j will converge...\n",
    "        \n",
    "scikit kmeans\n",
    "    create clustering model, but notice that the fit argument does not take a second argument with the target labels\n",
    "        specify the desired number of clusters (hyper parameter) with the n_clusters argument\n",
    "        It will automatically\n",
    "    attrubutes the model sets\n",
    "        cluster_centers_ : the center of each cluster in feature space\n",
    "        labels_:the cluster that each training instance was assigned to\n",
    "        something_else_he_skipped_\n",
    "        \n",
    "    \n",
    "\n",
    "Quiz today\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas\n",
    "#ex: \n",
    "#new_instance = [[feature1, feature2, ...]]\n",
    "\n",
    "def build_cluster_data(centroids, noise_sd, points_per_cluster, random_state=20):\n",
    "    feature_names = [\"X\", \"Y\"]\n",
    "    \n",
    "    #generate the same sequence eqch time\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    \n",
    "    grouping = zip(centroids, noise_sd, points_per_cluster)\n",
    "    for ((x,y), (x_sd, y_sd), points) in grouping:\n",
    "        x_noise_values = np.random.normal(0, x_sd, points)\n",
    "        y_noise_values = np.random.normal(0, y_sd, points)\n",
    "        \n",
    "        x_values = x+x_noise_values\n",
    "        y_values = y+y_noise_values\n",
    "        \n",
    "        data_x = np.concatenate((data_x, x_values))\n",
    "        data_y = np.concatenate((data_y, y_values))\n",
    "        \n",
    "        #create a dictionary\n",
    "        #placeholder\n",
    "        #He is skipping lots of code\n",
    "        \n",
    "#def build_rings:\n",
    "        \n",
    "#examples\n",
    "\n",
    "#construct some data\n",
    "\n",
    "centroids = [(1,4),(4,1)]\n",
    "noise = [(.5, 3.0), (.5, 1.5)]\n",
    "points = [150, 250]\n",
    "data_frame = build_cluster_data(centroids, noise, points, random_state=45)\n",
    "data_frame.head()\n",
    "\n",
    "#he is skipping lots of code because he has it written down and doesn't GAF if students do\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3/18/2021\n",
    "\"\"\"\n",
    "MNIST data set, contains 70k 28 pixel images. Each pixel is a grey scale value from 0 (white) to 255(black). The images are hand drawn\n",
    "images of the digits from 0-9 with the correct classification\n",
    "\n",
    "This data set is often stored as a single set of 70k images. In that case, by convention, first 60k are training set, remaining 10k are test set\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "opening-metadata",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-af753f5d7df5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#For each instance in x, return the classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#3/23/2021\n",
    "#classifier predictions\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state = 29)\n",
    "model = SGDClassifier(random_state = 29, max_iter = 20, tol=0.1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model.predict(x) #For each instance in x, return the classification\n",
    "model.predict_proba(x) #For each instance in x we return an array of probabilities \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Try out on some of the images\n",
    "\n",
    "print(mode.predict([x_train[59000]]))\n",
    "print(mode.predict([x_train[59001]]))\n",
    "print(mode.predict([x_train[59002]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lyric-certificate",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-af753f5d7df5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#For each instance in x, return the classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#3/23/2021\n",
    "#classifier predictions\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state = 29)\n",
    "model = SGDClassifier(random_state = 29, max_iter = 20, tol=0.1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model.predict(x) #For each instance in x, return the classification\n",
    "model.predict_proba(x) #For each instance in x we return an array of probabilities \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Try out on some of the images\n",
    "\n",
    "print(mode.predict([x_train[59000]]))\n",
    "print(mode.predict([x_train[59001]]))\n",
    "print(mode.predict([x_train[59002]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-papua",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "    often want to look at the data in our problem and try to determine which features or comibnations of freatures are most important and ignore features that are less important\n",
    "    \n",
    "    for example if we look at the 28x28 pixel images from MNIST the pixels on the borders are almost all white and do not contribute to the decision process, in addition neighboring pixels are highly correlated. We could consider replacing some pairs with their average\n",
    "    \n",
    "    Problems with high demnsions\n",
    "    1. training is slower\n",
    "    2. space to store data is a problem\n",
    "    3. make it harder to find a good solution\n",
    "    4. Harder to visualize the problem\n",
    "    Problems with reduction\n",
    "    1. performance may be decreased\n",
    "\n",
    "Extreme points\n",
    "\n",
    "suppose that we have a unit line and we pick a random point uniformly. What is probability that the point is close (Ex 5%) to the edge? It is proportional to the length of the border, in this case 1-0.9 = .1\n",
    "\n",
    "Distance in a hypercube\n",
    "\n",
    "if we pick two random points in a unit square, the average distance is about .52\n",
    "\n",
    "If we pick two random points in a unit cube, the average distance is .66\n",
    "\n",
    "\n",
    "In the L2 norm we compute the difference in each dimension, square each of the differences, add and then take a square root. Each of the differences in the unit hypercurve are less than one, but we may have a lot of them. This means that training sets for high-dimention problems may be very sparse (training instances are likely to be far away from each other)\n",
    "\n",
    "this means that predicitons are less likely to be relaible\n",
    "\n",
    "we might want to fix the problem by increasing the number of training instances so that we achieve a given density of training instances (more data is one of our standard approaches to improve performance). Unfortunately the number of instances required is exponential in the number of dimensions. With just 100 features we need more instances than atoms in the universe to require that instances are within 0.1 of each other\n",
    "\n",
    "\n",
    "Projection\n",
    "Mathematical transformation that converts from an N-dimensional space to a smaller space.\n",
    "example: convert a 3 dimensional object to a 2-dimensional image (camera)\n",
    "\n",
    "Example: Convert a 4-dimensional data to a 2=dimensional data by projecting onto a plane (we did a simple version of this when we dropped two of the four features of the IRIS data)\n",
    "\n",
    "\n",
    "Manifold\n",
    "\n",
    "a lower dimensional surface embedded in a higher dimensional space\n",
    "\n",
    "ex: surface of a sphere. If we have 3d coodinates of points on the earth we can reduce to the underlying 2d coordinates such as latitude and longitude\n",
    "\n",
    "Trying to convert from the manifold to a plane involves some distorion, here is miller cylindrical\n",
    "or a swiss roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-british",
   "metadata": {},
   "source": [
    "## start 3/25/2021 notes\n",
    "\n",
    "PCA\n",
    "    one popular method of reducing the dimensionality of a problem is to use Principle Component Analysis\n",
    "    Steps:\n",
    "        identify the hyperplane that lies closest to the data\n",
    "        Project the data on that hyperplane\n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        - suppose we have 3D data and we want to reduce the dimensionality to one. This means that we want to find the best line through the data, then project the data onto the line. Classic image: a scatter plot with a general y=x shape. Make a cross diagonally to include most of the points\n",
    "        \n",
    "        - We want to maximize the variance of the data nad minimizes the distance of the datapoints from that line. The idea is that we want to preserver large differences in the data as they are more likely to be determinative of the result.\n",
    "        \n",
    "        -Identify the axis with maximum variation which is called the first principle component, this will be some combinations of all the dimensions. It also identifies second, third, etc. principle components. Each of these will be perpendicular to the other axes.\n",
    "        \n",
    "   For PCA, correlation of 1 is good, correlation of -1 is an anti correlation and is bad maybe\n",
    "   \n",
    "   Descriptive math\n",
    "       we can compute a covariance (correlation) matrix of data with N features (dimensions). This will be an N by N square matrix that identifies for each pair of features how strongly they are related. Zero is no correlation. Once we have a correlation matrix we can computer N Eigen vectors and Eigen values. Each Eigen vector is a princple component. The Eigen value identifies with Eig Vectors are the most important (larger values are more important.) We drop dimensions by projecting onto a subset of the Eigen vectors with the largest values. THere are cases of degeneracies where you won't find Eigen vectors\n",
    "       \n",
    "      How many dimensions?\n",
    "          One technique is to let the number of dimensions 'float' until the amount of explained variations add up to a sufficiently large amount of the total variance\n",
    "          \n",
    "          Kernel trick strikes again\n",
    "                 We can do use it with PCA to reduce data on non-linear manifolds (like swiss roll) to a lower dimensionality. We have familiar kernels like RBF and sigmoid\n",
    "                 \n",
    "                 \n",
    "         PCA is unsupervised. How do we know what the best kernel and hyper-parameter choices are? We don't have an obvious performance measure\n",
    "         \n",
    "         Typical solution: Do a grid search over the kernels and parameters and find the\n",
    "         \n",
    " Other techniques\n",
    " \n",
    " LLE, MDS, Isomap, t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2) #2 = target number of dimensions we want to go through\n",
    "X_2D = pca.fit(transform(x))\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "#Find all components\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "X_2D = pca.fit_transform(X)\n",
    "\n",
    "cumulative_sums = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#Find num of dimentions to explain 95% of varation\n",
    "d = np.argmax(cumulative_sums >= 0.95 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-newfoundland",
   "metadata": {},
   "source": [
    "### Biological Neural Net (BNN) aka your brain\n",
    "    \n",
    "    Your brain is a large collection of relatively simple processors. Intelligence seems to be an emergent property of the collection\n",
    "    \n",
    "    Once of the earliest ideas for implementing non-biological intelligence was to model the neurons in our brain.\n",
    "    \n",
    "    Look at biology: Neurons, synapses, cells, dendrites\n",
    "    \n",
    "    Artificial Neural Nets (ANN)\n",
    "    We will simulate neurons, more or less\n",
    "    \n",
    "    1943 McCulloch and Pitt. Propose a simplified model of how neurons might work together using propositional logic\n",
    "    \n",
    "    early success in 1960s but problems led to reduction of funding\n",
    "    \n",
    "    SVMs take off in 1990s and new issues with NN show up\n",
    "    \n",
    "    Renewed interest in NN, tweaks to training and initializiion improve results using multiplayer NN (deep learning)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-gambling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "matched-poker",
   "metadata": {},
   "source": [
    "# 3/30/2021\n",
    "\n",
    "McCulloch - Pitt logic functions\n",
    "We get an output if there are at least two signals\n",
    "\n",
    "Some inputs can inhibit the firing of a neuron (biologically possible)\n",
    "\n",
    "A and B, A or B, A and (not)B. Just like programming logic\n",
    "\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "Invented by Rosenblatt in 1957\n",
    "\n",
    "Inputs and outputs are numerical values\n",
    "\n",
    "A sum of the weighted inputs is computed and used as the input to a step function. here are a couple common step functions\n",
    "\n",
    "    Heaviside(z) = 0 if z<0, 1 if z >= 0\n",
    "    \n",
    "    Sgn(z) = -1 if z < 0, 0 if z = 0, 1 if z > 0\n",
    "    \n",
    "    Most of the time you will get 1 or -1\n",
    "    \n",
    "Threshold logic unit (TLU)\n",
    "    \n",
    "    Weighted sum of inputs\n",
    "    Activation function is a treshold. If the weighted sum is above the threshhold, you get a constant positive response. Otherwise response is zero\n",
    "    \n",
    "    output = step((w^t)x)\n",
    "    \n",
    "    One LTU can do a simple linear binary classification\n",
    "    \n",
    "        A perceptron is a single layer of LTU, each one connected to all input features. Sometimes, there is a layer of pass through neurons that \n",
    "        represent the inputs.\n",
    "        \n",
    "        We may add bias input (always 1)\n",
    "        \n",
    "        *graph of 3 inputs + bias and 2 outputs*\n",
    "        \n",
    "        8 weight values\n",
    "        \n",
    "Perceptron training\n",
    "\n",
    "    Hebbian learning. When one neuron frequently triggers another neuron, the connection between them is strengthened. \"Cells that fire together, wire \n",
    "    together\"\n",
    "    \n",
    "    Perceptrons get one training instance at a time\n",
    "    \n",
    "    Compare the predicted output with correct output. Every output neuron that gives a wrong reponse increawes the weight to input neurons that\n",
    "    would have contributed to a correct response\n",
    "    \n",
    "    The learning boundary of each output neuron is linear, so perceptrons are not capable of learning complex patterns\n",
    "    \n",
    "    XOR problem\n",
    "    \n",
    "    Training cont.\n",
    "        \n",
    "    i = 1. n input neurons (input features)\n",
    "    j = 1. m output neurons\n",
    "    \n",
    "    an n by m array of weights (W(i)(j))\n",
    "    \n",
    "    a training instance and target outputs (x,y)\n",
    "    \n",
    "    perceptron output for training instance y1 = step(summation i=1, upward n: w(i)(x(i)) where i's are subscript\n",
    "    \n",
    "    training weight fancy n\n",
    "    \n",
    "    how to train the model\n",
    "    \n",
    "        fancy equation using everything above\n",
    "        \n",
    "        \n",
    "    If we have K instances to train on, we feed them into the perceptron.\n",
    "    \n",
    "    \n",
    "    \n",
    "Multilayer NN\n",
    "\n",
    "    Next wave of NN popularity was when an efficient nmethod of training a multlayer NN was discovered. Backpropagation feeds an instance at a time through the neural net and computes a difference with he desired value. Those differences are propagated back through the net and weights are adjusted. It looks imilar to the training that we did for a perceptron. Unlike a perceptron we can now learn functions like XOR\n",
    "    \n",
    "    A multilayer NN will have one input pass through node for each of the features. It has one or more hidden layers and a final output layer. Each layer apart from the output layer may havea  bias node set to 1. Each layer is fully connected to the next layer\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-apple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
